![Microsoft Cloud Workshops](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/master/Media/ms-cloud-workshop.png "Microsoft Cloud Workshops")

<div class="MCWHeader1">
Operationalizing Azure Kubernetes Service
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
November 2020
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

Â© 2020 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at https://www.microsoft.com/legal/intellectualproperty/Trademarks/Usage/General.aspx are trademarks of the Microsoft group of companies. All other trademarks are property of their respective owners.

**Contents**

<!-- TOC -->

- [Trainer information](#trainer-information)
  - [Role of the trainer](#role-of-the-trainer)
  - [Whiteboard design session flow](#whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#before-the-whiteboard-design-session-how-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#during-the-whiteboard-design-session-tips-for-an-effective-whiteboard-design-session)
- [Operationalizing Azure Kubernetes Service whiteboard design session student guide](#operationalizing-azure-kubernetes-service-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study)
    - [Customer situation](#customer-situation)
    - [Customer needs](#customer-needs)
    - [Customer objections](#customer-objections)
    - [Infographic for common scenarios](#infographic-for-common-scenarios)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution)
    - [Business needs](#business-needs)
    - [Design](#design)
    - [Prepare](#prepare)
  - [Step 3: Present the solution](#step-3-present-the-solution)
    - [Presentation](#presentation)
  - [Wrap-up](#wrap-up)
  - [Additional references](#additional-references)
- [Operationalizing Azure Kubernetes Service whiteboard design session trainer guide](#operationalizing-azure-kubernetes-service-whiteboard-design-session-trainer-guide)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#step-3-present-the-solution-1)
  - [Wrap-up](#wrap-up-1)
  - [Preferred solution](#preferred-solution)

<!-- /TOC -->

# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.
- Stimulates the participant's thinking.
- Involves the participant in the learning process.
- Manages the learning process (on time, on topic, and adjusting to benefit participants).
- Ensures individual participant accountability.
- Ties it all together for the participant.
- Provides insight and experience to the learning process.
- Effectively leads the whiteboard design session discussion.
- Monitors quality and appropriateness of participant deliverables.
- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements
- Current customer infrastructure and architecture
- Potential issues, objectives and blockers

**Step 2: Design a proof of concept solution (60 minutes)**

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.
- Determine customer's business needs to address your solution.
- Design and diagram your solution.
- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

**Outcome**

Present solution to your customer:

- Present solution
- Respond to customer objections
- Receive feedback

**Wrap-up (15 minutes)**

- Review preferred solution

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.
- Become familiar with all key points and activities.
- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.
- Prior to the whiteboard design session, discuss the case study to pick up more ideas.
- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.
- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.
- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

***Have fun**! Encourage participants to have fun and share!*

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Operationalizing Azure Kubernetes Service whiteboard design session student guide

## Abstract and learning objectives

In this whiteboard design session, you will work in a group to design a comprehensive solution to address concerns about existing workloads hosted in Azure Kubernetes Service (AKS). Areas to address include:

- Compute performance as new workloads are shifted to AKS and performance of workloads with difference compute needs (*e.g.* storage, graphics, etc.)
- Segregation of duties and ensuring operations and development only have access to the cluster services and deployments related to their role
- Ensuring applications that are built against more current LTS versions of Ubuntu are used for cluster compute

At the end of this whiteboard design session, you will be better able to design and plan AKS deployments as well as how to operationalize one or more AKS deployments for your customers. Specifically, you will learn how to scale applications in AKS with varying compute needs, how to leverage modern authentication and Azure role-based access controls (RBAC) for cluster management, and how to customize AKS deployments beyond their default state, including deploying clusters on non-default operating systems.

## Step 1: Review the customer case study

### Outcome <!-- omit in toc -->

Analyze your customer's needs.

Time frame: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1. Meet your table participants and trainer.
2. Read all the directions for steps 1-3 in the student guide.
3. As a table team, review the following customer case study.

### Customer situation

One of your largest customers, Contoso Commerce, has approached you about taking on operations surrounding an existing customer-facing e-Commerce platform they developed that is composed of a series of microservices hosted in an existing AKS cluster. While the existing platform and underlying microservices meet the needs of the business, Contoso Commerce plans to grow their online presence significantly as more customers move to online ordering exclusively. They anticipate that growth will be organic and bursty.

In anticipation of this new area of growth, they are rapidly developing new applications as well to perform data analysis and reporting functions to enhance their business. They are also actively refactoring their existing logistics management solution to containers so that it too can be run in AKS and be more portable.

Contoso Commerce's existing AKS deployment is comprised of a single three-node AKS cluster with the following architectural elements:

![Current architecture](images/Slide6.PNG)

- `Standard_DS3_v2` virtual machines for the existing node pool members
- Kubernetes RBAC for access to the API plane of the cluster
- Ubuntu 16.04 operating system for cluster nodes

### Customer needs

Contoso Commerce is has approached you and your team to take over the management and operations of their Azure estate, and specifically the management and operations of AKS, including any updates that need to be made to the environment to ensure it is positioned for future growth. The Azure subscriptions Contoso Commerce uses today will remain and will continue to be bound to their existing Azure Active Directory (Azure AD) tenant. Developers for each application of Contoso Commerce will still be responsible for the deployment of their applications. You will provide operational support for the overall cluster to ensure it meeds the functional requirements and capacity needs of Contoso Commerce as well as providing on-demand support to application developers should they encounter issues with the operations of their applications/workloads.

You challenge is to determine how the existing workloads can be hosted in a cluster that meets the customer requirements. You must provide an architecture for the cluster, a detailed breakdown of the configuration items that will meet your customer requirements, and a roadmap to implementation of your new architecture. If any downtime is required, detail how that downtime can be mitigated.

With any implementation of AKS that you operate, the following areas must be addressed.

**Operator and developer access**

Contoso Commerce's developers will require access to the cluster to deploy their applications using traditional Kubernetes deployments and through Helm v3 charts. Each application has a distinct set of developers and they should not have access to the `default` or `kube-system` namespaces on the cluster. If they do require a service in one of these namespaces, they will need to work in tandem with you on that deployment.

Contoso Commerce maintains security groups on-premises in Active Directory that represent each development team and those groups are synchronized to Azure AD using Azure AD Connect. Developers must be provided access to the cluster and their dedicated namespaces through their existing identities, including usernames and security groups in their Azure AD tenant.

**Application onboarding**

A new application is coming online which will be developed by another team at Contoso Commerce and will perform analytics for data in the existing e-Commerce platform. The new application has high IOPS requirements that exceed the maximum IOPS of the existing `Standard_DS3_v2` cluster nodes. In addition, individual pods with the new application will need to be exposed through their private IPs from peered networks. Each node in the cluster today runs up to 100 pods comfortably with only the existing e-Commerce platform in place.

**Node operating system**

While the current e-Commerce application has been developed and validated against Ubuntu 16.04 LTS, all future applications such as the new analytics application have been built and tested on development environments targeting Ubuntu 18.04 LTS. Contoso Commerce will be moving their existing e-Commerce platform to Ubuntu 18.04 development cluster imminently and would like to proceed with AKS clusters based on Ubuntu 18.04 moving forward in Azure.

**Migration**

Existing applications must be migrated from any existing clusters and validated on any new clusters prior to certification for production use by Contoso Commerce. Deployments today are currently executed manually from a series of scripts that are stored in source control with each application. Deployments are mixed, with some teams using native Kubernetes deployments and others adopting Helm. DevOps teams at Contoso Commerce have also been exploring Rancher.

**Operations and monitoring**

With multiple applications coming online, each with different performance characteristics, per-application operational monitoring will be required and each development team needs access to a dashboard where they can view the performance of only their applications containers. In addition, Contoso Commerce's existing operations team will need read-only access to a dashboard to understand performance across the cluster.

**Scaling**

Due to the bursty nature of the existing workloads and to better accommodate future growth, horizontal scaling must be implemented for each application. Any scaling strategy must include support for both node and pod scaling.

### Customer objections

1. Minimal downtime must be achieved as a part of the implementation of a new cluster. The existing e-Commerce platform is accessed by customers globally and Contoso Commerce can only accommodate one hour of downtime.
   1. This must also be accounted for moving forward as new Kubernetes updates are released and/or other patching such as security updates needs to occur.
2. With operational responsibilities falling on your organization, Contoso Commerce is very concerned about whom will have access to their AKS cluster(s) and will require an audit trail for who has access to the cluster.
3. IT leadership is concerned about the prospect of an every growing AKS cluster. Constraints for scaling must be in place and when those constraints prevent deployments or operations IT leadership at Contoso Commerce must be informed.

### Infographic for common scenarios

<!-- Slide screenshots from WDS deck -->

**Common scenarios - Cluster identity**

![Common scenarios - Cluster identity](images/Slide14.PNG)

**Common scenarios - Networking**

![Common scenarios - Networking](images/Slide15.PNG)

**Common scenarios - Migration**

![Common scenarios - Migration](images/Slide16.PNG)

**Common scenarios - Scaling**

![Common scenarios - Scaling](images/Slide17.PNG)

**Common scenarios - Monitoring**

![Common scenarios - Monitoring](images/Slide18.PNG)

## Step 2: Design a proof of concept solution

### Outcome <!-- omit in toc -->

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Time frame: 60 minutes

### Business needs

Directions:  With all participants at your table, answer the following questions and list the answers on a flip chart:

1. Who should you present this solution to? Who is your target customer audience? Who are the decision makers?
2. What customer business needs do you need to address with your solution?

### Design

Directions: With all participants at your table, respond to the following questions on a whiteboard (physical or virtual):

**Operator and developer access**

1. What authentication method would you recommend for access to the API server in AKS through tooling like `kubectl`? What impacts does your selection have on the configuration of clusters going forward?
   1. If you alter the current identity model, how will you translate existing roles and cluster role bindings to the new identity model?
2. Are there any considerations for Helm v3 which will impact the configuration of your cluster?
3. How will you prevent developers in existing Azure AD security groups from accessing the `default` and `kube-system` namespaces?

**Application onboarding**

1. What types of compute would you recommend for applications with high IOPS requirements? If a future application or workload requires another type of compute how would you incorporate it into Contoso Commerce's cluster(s) and that the compute can only be utilized by the workload is was created to host?  How would you implement your solution in AKS?
2. What IP addressing scheme would you recommend to Contoso Commerce to best balance resource usage for nodes and pods?
   1. How did you accommodate future growth and cluster upgrades in your IP address scheme?
3. What considerations are there for network security with pods being exposed directly to the virtual network?

**Node operating system**

1. What considerations are there to deploy AKS clusters for Contoso Commerce with Ubuntu 18.04? Are there material impacts to the business if they move forward with their current plan?
2. Contoso Commerce will need a "go-forward" plan as new versions of Ubuntu become available. What approach would you recommend they take to perform rapid testing of new operating systems with support for Kubernetes and how would you implement your approach?

**Migration**

1. How will you migrate existing applications from the current cluster to a new cluster for validation?
2. How will you minimize downtime for currently running workloads?
3. What would you recommend to automate the execution of deployments based on the approach Contoso Commerce has used to-date?

**Operations and monitoring**

1. How will you collect and monitor cluster, node, and pod health and performance? Where will the logs be stored and how will they be accessed?
2. What will you use to visualize logs for each application team?
3. How will you ensure that access to logs is read-only?

**Scaling**

1. Describe your strategy for both node and pod scaling within the cluster(s). What metrics will you use for scaling? Does your scaling configuration have any impacts on the ability to upgrade your clusters in the future?
2. With a bursty application such as the e-Commerce platform, what scaling methodology will you employ to ensure that scale operations can happen as fast as possible?
  
### Prepare

Directions: With all participants at your table:

1. Identify any customer needs that are not addressed with the proposed solution.
2. Identify the benefits of your solution.
3. Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

### Outcome <!-- omit in toc -->

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Time frame: 30 minutes

### Presentation

**Directions**

1. Pair with another table.
2. One table is the Microsoft team and the other table is the customer.
3. The Microsoft team presents their proposed solution to the customer.
4. The customer makes one of the objections from the list of objections.
5. The Microsoft team responds to the objection.
6. The customer team gives feedback to the Microsoft team.
7. Tables switch roles and repeat Steps 2-6.

## Wrap-up

Time frame: 15 minutes

**Directions**

Tables reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

|||
|--------------------------------|:----------------------------------------------:|
| Access and identity options for Azure Kubernetes Service (AKS) | <https://docs.microsoft.com/azure/aks/concepts-identity> |
| Azure Kubernetes Service (AKS) | <https://docs.microsoft.com/azure/aks/intro-kubernetes> |
| Azure Monitor for containers overview | <https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-overview> |
| Checking for Kubernetes best practices in your cluster | <https://docs.microsoft.com/azure/aks/kube-advisor-tool> |
| Develop on Azure Kubernetes Service (AKS) with Helm | <https://docs.microsoft.com/azure/aks/quickstart-helm> |
| Migrate to Azure Kubernetes Service (AKS) | <https://docs.microsoft.com/azure/aks/aks-migration> |
| Network concepts for applications in Azure Kubernetes Service (AKS) | <https://docs.microsoft.com/azure/aks/concepts-network> |
| Scaling options for applications in Azure Kubernetes Service (AKS) | <https://docs.microsoft.com/azure/aks/concepts-scale> |
| Security concepts for applications and clusters in Azure Kubernetes Service (AKS) | <https://docs.microsoft.com/azure/aks/concepts-security> |
| Security hardening for AKS agent node host OS | <https://docs.microsoft.com/azure/aks/security-hardened-vm-host-image> |

# Operationalizing Azure Kubernetes Service whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your table participants to introduce yourself as the trainer.
- Ask, "What questions do you have about the customer case study?"
- Briefly review the steps and time frames of the whiteboard design session.
- Ready, set, go! Let the table participants begin.

## Step 2: Design a proof of concept solution

- Check in with your tables to ensure that they are transitioning from step to step on time.
- Provide some feedback on their responses to the business needs and design.
  - Try asking questions first that will lead the participants to discover the answers on their own.
- Provide feedback for their responses to the customer's objections.
  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which table will be paired with your table before Step 3 begins.
- For the first round, assign one table as the presenting team and the other table as the customer.
- Have the presenting team present their solution to the customer team.
  - Have the customer team provide one objection for the presenting team to respond to.
  - The presentation, objections, and feedback should take no longer than 15 minutes.
  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have the table participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred solution

**Operator and developer access**

1. **Design:** What authentication method would you recommend for access to the API server in AKS through tooling like `kubectl`? What impacts does your selection have on the configuration of clusters going forward?
  
    **Solution:** Azure Kubernetes Service (AKS) can be configured to use Azure Active Directory (Azure AD) for user authentication. In this configuration, users can sign in to an AKS cluster by using your Azure AD authentication token.

    Cluster administrators can configure Kubernetes role-based access control (RBAC) based on a user's identity or directory group membership.

    This is the ideal solution as Contoso Commerce has expressed they would like to grant existing users and security groups in their Azure AD access to the cluster to perform day-to-day activities (*e.g.* deployments for a workload by an application developer).

   1. **Design:** If you alter the current identity model, how will you translate existing roles and cluster role bindings to the new identity model?

      **Solution:** Discovery will need to be performed on the current cluster to determine what security principals have access to the cluster and which namespaces those principals have access to. Discovery could be extensive and should be accounted for.

2. **Design:** Are there any considerations for Helm v3 which will impact the configuration of your cluster?

    **Solution:** For RBAC enabled clusters, Helm v2 required a service account and rolebinding for the Tiller service which required granting the Tiller service account access to the namespace(s) where Helm would be used to deploy resources (see [Helm role-based access controls](https://v2.helm.sh/docs/using_helm/#role-based-access-control) for more information). Helm v2 was also based on a client/server architecture which required that even for non-RBAC enabled clusters that Tiller be installed.

    Helm v3 removes this requirement. The client/server architecture has been replaced with a client/library architecture where only the `helm` binary is required. This means that security is now on a per-user basis and effectively delegated to the Kubernetes cluster which is the target of the deployment (see [Migrating Helm v2 to v3](https://helm.sh/docs/topics/v2_v3_migration/) for more information.)

3. **Design:** How will you prevent developers in existing Azure AD security groups from accessing the `default` and `kube-system` namespaces?

    **Solution:** As Contoso Commerce continues to grow and new container-based applications and workloads are developed, they will need to not only have a clear separation in who has access to what in the cluster, but also making sure that each team has a "safe place" to deploy their workloads without impacting existing services becomes critical. For each application/workload that is developed, a dedicated namespace will be created and that development team's associated Azure AD security group will be granted access to that namespace (and only that namespace).

    To ensure that one team cannot use more resources than they should be, [Resource Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) should be created for each namespace and reevaluated periodically to ensure that they meet the needs of the business.

    Overtime, it may even be preferable to extend the namespace model further and provide each development team multiple namespaces that map to the development lifecycle, each with its own unique resource quotas. For example, the e-Commerce team could be provided a namespace for not only their production deployments, but also development and staging depending on their needs.

    Note that while namespaces are abstracted from each other (*.e.g* hidden) they are not isolated by default. Services in one namespace can communicate with services in another namespace, even if the service shares the same name in multiple namespaces (see [Kubernetes best practices: Organizing with Namespaces](https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-organizing-with-namespaces)).

**Application onboarding**

1. **Design:** What types of compute would you recommend for applications with high IOPS requirements? If a future application or workload requires another type of compute how would you incorporate it into Contoso Commerce's cluster(s) and that the compute can only be utilized by the workload is was created to host? How would you implement your solution in AKS?

    **Solution:** IOPS requirements can be solved for in several ways in Azure, including the application of premium disks, striping slower disks, or even selecting a VM series that is designed for high IO. All of these options have various trade-offs, such as operational overhead, cost, and more.

    In this case, we know that the existing application is performed as desired within a default AKS configuration. For the new application, we can create a new node pool which is tuned specifically to that application. Before we can create a new node pool, we must select a VM series and size that we will use for our node pool members:

    - There are several [VM SKUs which should be avoided in AKS](https://docs.microsoft.com/azure/aks/quotas-skus-regions#restricted-vm-sizes) that can immediately be ruled out.
    - The [Sizes for Linux virtual machines in Azure](https://docs.microsoft.com/azure/virtual-machines/linux/sizes) offers a number of options, with the [storage optimized](https://docs.microsoft.com/azure/virtual-machines/sizes-storage) *[Lsv2-series](https://docs.microsoft.com//azure/virtual-machines/lsv2-series)* being a natural candidate as it is tuned for high disk throughput and transactional workloads such as data analytics.
    - Another decision point for selecting a VM SKU and size is availability of the SKU in a given region. In this case, we would want to ensure that both AKS *and* the selected VM SKU are *both* available in the region(s) we want to operate in.

    Once we have selected our size and series, we can create a new node pool in the cluster (see [Limitations](https://docs.microsoft.com/azure/aks/use-multiple-node-pools#limitations) for multiple node pool clusters for other considerations).

    ```sh
    az aks nodepool add \
      --resource-group constosAKSrg \
      --cluster-name constosoAKScluster \
      --name iopspool \
      --node-count 3 \
      --kubernetes-version 1.18.8
      --node-vm-size Standard_L8s_v2 \
    ```

    **Note:** For further isolation at the network layer, additional node pools can be split into their own subnet using the `--vnet-subnet-id <YOUR_SUBNET_RESOURCE_ID>` argument on the `az aks nodepool create` command.

    **Note:** If the `--node-vm-size` argument is not supplied at the time the node pool is created, the node members will be created in a default size (`Standard_DS2_v2` for Linux node pools and `Standard_DS3_v2` for Windows node pools).

    To restrict the new compute to just the desired workload(s), taints will need to be applied to nodes and then tolerations applied to pods (see [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)).

    - A taint is applied to a node that indicates only specific pods can be scheduled on them.
    - A toleration is then applied to a pod that allows them to tolerate a node's taint.

    Taints can be applied to a node pool using the `--node-taints` parameter of the `az aks nodepool add` command. For example:

    ```sh
    az aks nodepool add \
      -g ...
      -n ...
      --node-taints iopspool sku=iops:NoSchedule
    ```

    Creating pods and specifying a toleration of `sku=iops:NoSchedule` will allow the pod to be scheduled on the specified node pool. For example:

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: iopspod
    spec:
      containers:
      - image: myImage
        name: iopspod
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 1
            memory: 2G
      tolerations:
      - key: "sku"
        operator: "Equal"
        value: "iops"
        effect: "NoSchedule"
    ```

2. **Design:** What IP addressing scheme would you recommend to Contoso Commerce to best balance resource usage for nodes and pods?

    **Solution:**  When the new cluster is provisioned, it will need to be configured for Azure CNI networking (see [Configure Azure CNI networking in Azure Kubernetes Service (AKS)](https://docs.microsoft.com/azure/aks/configure-azure-cni)) so that each pod can be exposed directly to the subnet directly through a private IP.

    Creating a new cluster with CNI will require several pieces of information:

    - **Virtual network** with defined address range.
    - **Subnet** with defined address range. The subnet must be large enough to accommodate the nodes, pods, and all Kubernetes and Azure resources that need to be deployed with your cluster. This includes resources such as internal Azure Load Balancer(s).
    - **Kubernetes service address range** which is not used by any network element connected to the **virtual network**.
    - **Kubernetes DNS service IP address** which defines an IP address within your **Kubernetes service address range** that is used for cluster discovery.
    - **Docker bridge address** which represents the default *docker0* bridge present in all Docker installations. This must be a CIDR range that does not collide with the rest of the CIDRs on your network, including the cluster and pod CIDR ranges.

    To calculate the minimum node and pod subnet size including an additional node for upgrade operations:

    ```sh
    (number of nodes + 1) + ((number of nodes + 1) * maximum pods per node that you configure)
    ```

    Consider that each node in the cluster today runs up to 100 pods comfortably with only the existing e-Commerce platform in place. To accommodate just the existing cluster and its default node pool assuming 100 pods per node:

    ```sh
    ((3 + 1) + ((3 + 1) * 110) = 444 IPs required
    ```

    **Note:** Even though 100 pods run the existing workload, we want to ensure there is slight overhead for system pods. 10 was used here as that is the minimum value of maximum number of pods per node when using Azure CNI (see [Configure maximum - new clusters](https://docs.microsoft.com/azure/aks/configure-azure-cni#configure-maximum---new-clusters)).

    As we do not know the IP addressing requirements of the new data analytics workload, we will assume the same number of pods per node (even though those will be running on a different node pool). This would mean we need another *444* IPs for another 3 node pool, bringing our total cluster nodes to 6 (3 per node pool) and our minimum IP requirements to *888*. With the minimal requirements, we would need an IP range of *at least* a `/22`:

    ```sh
    10.0.0.0/22 = 10.0.0.0 - 10.0.3.255 (1,022 addresses)
    ```

    Of course, this does limit us for future growth as we only have *134* IPs remaining. To allow for meaningful future growth, it is recommended to use at least a `/21`.

    ```sh
    10.0.0.0/21 = 10.0.0.0. - 10.0.7.255 (2046 addresses)
    ```

    Once you know all of your address ranges, you can proceed to create your virtual network. Once your virtual network resource has been provisioned and you have retrieved the resource ID of the subnet you want to use for IP allocation to nodes and pods you can create your cluster:

    ```sh
    # retrieve the subnet resource. e.g.:
    # /subscriptions/<guid>/resourceGroups/constosAKSrg/providers/Microsoft.Network/virtualNetworks/constosAKSvnet/subnets/default
    subnet_id=az network vnet subnet list \
      --resource-group constosAKSrg \
      --vnet-name constosAKSvnet \
      --query "[0].id" --output tsv

    az aks create \
      --resource-group constosAKSrg \
      --name constosoAKScluster \
      --network-plugin azure \
      --vnet-subnet-id $subnet_id \
      --docker-bridge-address 172.17.0.1/16 \
      --dns-service-ip 10.2.0.10 \
      --service-cidr 10.2.0.0/24 \
      --generate-ssh-keys
    ```

   1. **Design:** How did you accommodate future growth and cluster upgrades in your IP address scheme?

      **Solution:** The number of IP addresses required should include considerations for upgrade and scaling operations. If you set the IP address range to only support a fixed number of nodes, you cannot upgrade or scale your cluster.

3. **Design:** What considerations are there for network security with pods being exposed directly to the virtual network?

    **Solution:** There are a number of considerations for securing not only your pods, but also the traffic between those pods. All pods in an AKS cluster can send and receive traffic without limitations, by default. Network Policy in Kubernetes will allow us to define rules for ingress and egress within the cluster.

    Network Policy is a Kubernetes specification that defines access policies for communication between pods. Using Network Policies, we define an ordered set of rules to send and receive traffic and apply them to a collection of pods that match one or more label selectors. Network policies can be defined in one-off manifests or as a part of a wider manifest which creates a deployment or a service.

    AKS supports two ways to implement network policy - *Azure Network Policies* or *Calico Network Policies* (see [Differences between Azure and Calico policies and their capabilities](https://docs.microsoft.com/azure/aks/use-network-policies#differences-between-azure-and-calico-policies-and-their-capabilities)).

    Network Policy and the selection of Azure or Calico policy must be selected at the time the cluster is created and cannot be added later. This can be done with the `--network-policy` argument of the `az aks create` command. Extending the command we used to create the cluster earlier with CNI, we would end up with:

    ```sh
    az aks create \
      --resource-group constosAKSrg \
      --name constosoAKScluster \
      --network-plugin azure \
      --vnet-subnet-id $subnet_id \
      --docker-bridge-address 172.17.0.1/16 \
      --dns-service-ip 10.2.0.10 \
      --service-cidr 10.2.0.0/24 \
      --generate-ssh-keys \
      --network-policy azure
    ```

    After the cluster is provisioned, traffic rules can be targeted to namespaces, allowing for flexibility in policy enforcement on a workload-by-workload basis.

    For more information, see [Secure traffic between pods using network policies in Azure Kubernetes Service (AKS)](https://docs.microsoft.com/azure/aks/use-network-policies).

**Node operating system**

1. **Design:** What considerations are there to deploy AKS clusters for Contoso Commerce with Ubuntu 18.04? Are there material impacts to the business if they move forward with their current plan?

    **Solution:** Node pools created on Kubernetes v1.18 or greater default to AKS Ubuntu 18.04 node image. Node pools on a supported Kubernetes version less than 1.18 receive AKS Ubuntu 16.04 as the node image, but will be updated to AKS Ubuntu 18.04 once the node pool Kubernetes version is updated to v1.18 or greater.

It is highly recommended to test workloads on AKS Ubuntu 18.04 node pools prior to using clusters on 1.18 or greater. 

    Also consider that in a default configuration, AKS does not come with an SLA which is a risk for any implementation, perhaps even more so for a service provider who is providing services around the management of AKS for customers.

    Starting March 2020, an [Azure Kubernetes Service (AKS) Uptime SLA](https://azure.microsoft.com/support/legal/sla/kubernetes-service/v1_1/) is available for purchase. This is a financially backed SLA and provides for availability of 99.95% for the Kubernetes API Server for AKS clusters that use Azure Availability Zones and 99.9% for AKS clusters that do not use Azure Availability Zones. The availability of the agent nodes of AKS Clusters is covered by the Virtual Machines SLA.

    It is important to recognize the distinction between AKS service availability which refers to uptime of the Kubernetes control plane and the availability of a specific workload which is running on Azure VMs. Although the control plane may be unavailable if the control plane is not ready, your cluster workloads running on Azure VMs can still function.

2. **Design:** Contoso Commerce will need a "go-forward" plan as new versions of Ubuntu become available. What approach would you recommend they take to perform rapid testing of new operating systems with support for Kubernetes and how would you implement your approach?

    **Solution:** As new versions of Ubuntu are released in a preview state, new clusters can be provisioned in Azure which target the new OS and testing can be performed against them.

    It is also possible to test new operating systems before they are available in the formal AKS service by leverage the underlying deployment and provisioning engine that Microsoft uses for AKS - [AKS Engine](https://github.com/Azure/aks-engine):

    > *AKS Engine provides convenient tooling to quickly bootstrap Kubernetes clusters on Azure. By leveraging ARM (Azure Resource Manager), AKS Engine helps you create, destroy and maintain clusters provisioned with basic IaaS resources in Azure. AKS Engine is also the library used by AKS for performing these operations to provide managed service implementations.*

**Migration**

1. **Design:** How will you migrate existing applications from the current cluster to a new cluster for validation?

    **Solution:** There are number of circumstances which will require a new AKS cluster outside of even this migration from the customers current cluster to a new cluster with the features that they require which you manage.

    In general, you can be better positioned to lessen the number of future migrations by adopting AKS features such as clusters backed by Virtual Machine Scale Sets (VMSS) and Azure Standard Load Balancer to ensure you get access to the latest features such as multiple node pools and [Azure Policy](https://docs.microsoft.com/azure/governance/policy/concepts/rego-for-aks).

    In regard to specific migration, you know that deployments today are currently executed manually from a series of scripts that are stored in source control with each application. Deployments are mixed, with some teams using native Kubernetes deployments and others adopting Helm. With manual deployments already authored, you can use the existing deployment scripts to target the new environment and modify them as required.

2. **Design:** How will you minimize downtime for currently running workloads?

    **Solution:** For many applications, especially complex applications that are critical to the success of the business, it may be necessary to migration application components over time. If the migration can not be executed within the window provided by the business, it will be necessary to stand up the new cluster and begin migrating individual services and deployments.

    This will likely require the implementation of a [load-balancing service](https://docs.microsoft.com/azure/aks/aks-migration#high-availability-and-business-continuity) that is external to the cluster such as Azure Application Gateway, Azure Traffic Manager, or Azure Front Door Service.

    ![AKS with Azure Traffic Manager in two regions](images/aks-azure-traffic-manager.png)

    Ultimately, you will need to undertake an assessment of the time it takes to deploy the workloads required and perform any data migrations to further refine your approach.

3. **Design:** What would you recommend to automate the execution of deployments based on the approach Contoso Commerce has used to-date?

    **Solution:** It would be appropriate to suggest that Contoso Commerce consider implementing Continuous Integration/Continuous Deployment (CI/CD) using Azure Pipelines (see [Build and deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/devops/pipelines/ecosystems/kubernetes/aks-template?view=azure-devops)) or GitHub Actions (see [Deploy to Kubernetes cluster](https://github.com/marketplace/actions/deploy-to-kubernetes-cluster)). Any investments in deployment automation now will greatly increase the speed at which new clusters can be deployed in the future - and new clusters will need to be deployed in the future as new features become available that can not be enabled on the existing cluster.

**Operations and monitoring**

1. **Design:** How will you collect and monitor cluster, node, and pod health and performance? Where will the logs be stored and how will they be accessed?

    **Solution:** Telemetry from the cluster, cluster nodes, and pods will be consolidated within a Log Analytics workspace. This will require the implementation of [Azure Monitor for containers](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-overview).

    Azure Monitor for containers gives you performance visibility by collecting memory and processor metrics from controllers, nodes, and containers that are available in Kubernetes through the Metrics API. Container logs are also collected. After you enable monitoring from Kubernetes clusters, metrics and logs are automatically collected for you through a containerized version of the Log Analytics agent for Linux. Metrics are written to the metrics store and log data is written to the logs store associated with your Log Analytics workspace.

    To [enable Azure Monitor for containers](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-onboard) you will need a [Log Analytics workspace](https://docs.microsoft.com/azure/azure-monitor/log-query/log-query-overview) and you must be an [Owner](https://docs.microsoft.com/azure/role-based-access-control/built-in-roles#owner) of the AKS cluster resource.

    The solution requires the deployment of a containerized Log Analytics agent developed specifically for Azure Monitor for containers. The specialized agent collects performance and event data from all nodes in the cluster, and the agent is automatically deployed and registered with the specified Log Analytics workspace during deployment. The agent version is `microsoft/oms:ciprod04202018` or later, and is represented by a date in the format `mmddyyyy`.

    Monitoring can be enabled on existing clusters or at the time a new cluster is created.

    Once monitoring is enabled and the agent deployed, you can begin to [monitor the performance of the AKS cluster](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-analyze) in Azure Monitor for containers or performance can be [viewed directly from a cluster](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-analyze#view-performance-directly-from-a-cluster).

    With Azure Monitor for containers the operational data is stored in Log Analytics. This means that you will need to not only need to provision a workspace, but also configure that workspace for any other solutions you desire. You may also need to configure retention for the workspace. Many configuration changes you make to a workspace will affect the cost of the underlying workspace (see [Estimating the costs to manage your environment](https://docs.microsoft.com/azure/azure-monitor/platform/manage-cost-storage#estimating-the-costs-to-manage-your-environment)).

2. **Design:** What will you use to visualize logs for each application team?

    **Solution:** While the primary visualization tool for operations will likely be Azure Monitor for containers, with multiple applications coming online each with different performance characteristics, per-application operational monitoring is a customer requirement and each development team needs access to a dashboard where they can view the performance of only their applications containers.

    For application teams, [Workbooks](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-analyze#workbooks) can be created which combine log queries, metrics, and parameters into interactive reports.

    Azure Monitor for containers includes four workbooks and custom workbooks can be created as well:

    - **Disk capacity:** Presents interactive disk usage charts for each disk presented to the node within a container by the following perspectives:
      - Disk percent usage for all disks.
      - Free disk space for all disks.
      - A grid that shows each node's disk, its percentage of used space, trend of percentage of used space, free disk space (GiB), and trend of free disk space (GiB).
    - **Disk IO:** Presents interactive disk utilization charts for each disk presented to the node within a container by the following perspectives:
      - Disk I/O summarized across all disks by read bytes/sec, writes bytes/sec, and read and write bytes/sec trends.
      - Eight performance charts show key performance indicators to help measure and identify disk I/O bottlenecks.
    - **Kubelet:** Includes two grids that show key node operating statistics:
      - Overview by node grid summarizes total operation, total errors, and successful operations by percent and trend for each node.
      - Overview by operation type summarizes for each operation the total operation, total errors, and successful operations by percent and trend.
    - **Network:** Presents interactive network utilization charts for each node's network adapter, and a grid presents the key performance indicators to help measure the performance of your network adapters.

3. **Design:** How will you ensure that access to logs is read-only?

    **Solution:** Contoso Commerce's existing operations team requires read-only access to a dashboard to understand performance across the cluster. To meet this need, log data will not only be retained to meet the needs of the customer, but Azure role-based access control will be used to set  permissions.

    There are currently several ways to grant access to a workspace and the data within it:

    - Workspace permissions (see [Manage access using workspace permissions](https://docs.microsoft.com/azure/azure-monitor/platform/manage-access#manage-access-using-workspace-permissions)).
    - Azure role-based access control for accessing data from:
      - Specific resources (see [Manage access using Azure permissions](https://docs.microsoft.com/azure/azure-monitor/platform/manage-access#manage-access-using-azure-permissions))
      - Specific tables (see [Table level RBAC](https://docs.microsoft.com/azure/azure-monitor/platform/manage-access#table-level-rbac))

    Members of the operations team can be assigned the *Log Analytics Reader* role on the AKS cluster resource and the Log Analytics workspace that contains their operational data. They can also be granted access to one or more workbooks to view the underlying dataset.

    By default, the data in Log Analytics is read-only and is only purged when the retention period for the data has expired. It is possible to purge data from a workspace using the *purge* API path, however this is a privileged operation and requires a specialized role assignment - *Data Purger* (see [How to export and delete private data](https://docs.microsoft.com/azure/azure-monitor/platform/personal-data-mgmt#how-to-export-and-delete-private-data).

**Scaling**

1. **Design:** Describe your strategy for both node and pod scaling within the cluster(s). What metrics will you use for scaling? Does your scaling configuration have any impacts on the ability to upgrade your clusters in the future?

    **Solution:** Node scaling will use the [cluster autoscaler](https://docs.microsoft.com/azure/aks/cluster-autoscaler#about-the-cluster-autoscaler) while pods will be scaled within node pools using the [horizontal pod autoscaler](https://docs.microsoft.com/azure/aks/tutorial-kubernetes-scale#autoscale-pods).

    **Cluster autoscaler**

    To configure the cluster autoscaler, the feature must first be enabled using the `--enable-cluster-autoscaler` parameter of the `az aks create` command. For example:

    ```sh
    az aks create \
      --resource-group constosAKSrg \
      --name constosoAKScluster \
      --network-plugin azure \
      --vnet-subnet-id $subnet_id \
      --docker-bridge-address 172.17.0.1/16 \
      --dns-service-ip 10.2.0.10 \
      --service-cidr 10.2.0.0/24 \
      --generate-ssh-keys \
      --network-policy azure \
      --enable-cluster-autoscaler
    ```

    Configuration of the cluster autoscaler varies based on whether or not single node or multi-node pools are being used.

    When using a single node pool, the cluster autoscaler minimum and maximum node counts as well as autoscaler profiles can be configured using `az aks update`:

    ```sh
    az aks update \
      --resource-group constosAKSrg \
      --name constosoAKScluster \
      --update-cluster-autoscaler \
      --min-count 3 \
      --max-count 10
    ```

    With multiple node pools, you must use the `az aks nodepool update` command:

    ```sh
    az aks nodepool update \
      --resource-group constosAKSrg \
      --cluster-name constosoAKScluster \
      --name iopspool \
      --update-cluster-autoscaler \
      --min-count 3 \
      --max-count 5
    ```

    Once cluster autoscaling has been enabled, you can further refine the configuration using autoscaler profiles. The profiles have a default configuration and are stored at the cluster level. Autoscaler profiles effect all node pools that have the cluster autoscaler enabled.

    The available profiles are:

    | Setting | Description | Default value |
    | ------ | ----------- | ------------- |
    | **scan-interval** | How often cluster is reevaluated for scale up or down | 10 seconds |
    | **scale-down-delay-after-add** | How long after scale up that scale down evaluation resumes | 10 minutes |
    | **scale-down-delay-after-delete** | How long after node deletion that scale down evaluation resumes | scan-interval |
    | **scale-down-delay-after-failure** | How long after scale down failure that scale down evaluation resumes | 3 minutes |
    | **scale-down-unneeded-time** | How long a node should be unneeded before it is eligible for scale down | 10 minutes |
    | **scale-down-unready-time** | How long an unready node should be unneeded before it is eligible for scale down | 20 minutes |
    | **scale-down-utilization-threshold** | Node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down | 0.5 |
    | **max-graceful-termination-sec** | Maximum number of seconds the cluster autoscaler waits for pod termination when trying to scale down a node. | 600 seconds |
    | **balance-similar-node-groups** | Detect similar node pools and balance the number of nodes between them | false |

    To set a value for a cluster autoscaler profile, you need to use the `aks-preview` Azure CLI extension:

    ```sh
    # Install the aks-preview extension
    az extension add --name aks-preview

    # Update the extension to make sure you have the latest version installed
    az extension update --name aks-preview
    ```

    You can then use `az aks update` with the `--cluster-autoscaler-profile` parameter to set values as required:

    ```sh
    az aks update \
      --resource-group constosAKSrg \
      --name constosoAKScluster \
      --cluster-autoscaler-profile scan-interval=30s
    ```

    Remember, when you upgrade your AKS cluster, a new node is deployed into the cluster. Services and workloads begin to run on the new node, and an older node is removed from the cluster. This rolling upgrade process requires a minimum of one additional block of IP addresses to be available - making your node count is then n + 1. You must have enough IP addresses available if using Azure CNI.

    **Horizontal pod autoscaler**

    Kubernetes supports [horizontal pod autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) to adjust the number of pods in a deployment depending on CPU utilization or other select metrics. The [Metrics Server](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server) is used to provide resource utilization to Kubernetes, and is automatically deployed in AKS clusters today.

    To use the autoscaler, all containers in your pods and your pods must have CPU requests and limits defined. For example:

    ```yaml
    resources:
      requests:
        cpu: 250m
      limits:
        cpu: 500m
    ```

2. **Design:** With a bursty application such as the e-Commerce platform, what scaling methodology will you employ to ensure that scale operations can happen as fast as possible?

    **Solution:** To rapidly scale your AKS cluster, you can integrate with Azure Container Instances (ACI). Kubernetes has built-in components to scale the replica and node count. However, if your application needs to rapidly scale, the horizontal pod autoscaler may schedule more pods than can be provided by the existing compute resources in the node pool. If configured, this scenario would then trigger the cluster autoscaler to deploy additional nodes in the node pool, but it may take a few minutes for those nodes to successfully provision and allow the Kubernetes scheduler to run pods on them.

    ![Architecture diagram of AKS cluster bursting into Azure Container Instances](images/burst-scaling.png)

    ACI lets you quickly deploy container instances without additional infrastructure overhead. When you connect with AKS, ACI becomes a secured, logical extension of your AKS cluster. The virtual nodes component, which is based on Virtual Kubelet, is installed in your AKS cluster that presents ACI as a virtual Kubernetes node. Kubernetes can then schedule pods that run as ACI instances through virtual nodes, not as pods on VM nodes directly in your AKS cluster.
